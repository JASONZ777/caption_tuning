{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMPwWZPd9zEilKzO5Ofhyu1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2rXxQzddUeU9","executionInfo":{"status":"ok","timestamp":1713463320611,"user_tz":240,"elapsed":10075,"user":{"displayName":"Junyu Zhou","userId":"10946542437674301844"}},"outputId":"c8953cfd-ffcd-4259-c9ee-3966a8058605"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/MyDrive/instruct-caption')"]},{"cell_type":"code","source":["!pip install -q transformers"],"metadata":{"id":"PfWQoaBhUr1s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import json"],"metadata":{"id":"EZ7vTIyeUmfU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["peft_model_id = \"saved_model\"\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"Qwen/Qwen1.5-1.8B-Chat\",\n","    torch_dtype=\"auto\",\n","    device_map=\"auto\"\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\")\n","\n","model.load_adapter(peft_model_id)\n","path = \"dataset.jsonl\"\n","with open(path, \"r\") as f:\n","  lst = [json.loads(line) for line in f.readlines()]\n","  correct = 0\n","  total = len(lst)\n","  for data in lst:\n","    user_prompt = data['input']\n","    system_prompt = data['instruction']\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_prompt}\n","    ]\n","    text = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","    generated_ids = model.generate(\n","        model_inputs.input_ids,\n","        max_new_tokens=512\n","    )\n","    generated_ids = [\n","        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","    if response == data['output']:\n","      correct+=1\n","\n","print(f\"Prediction accuracy is {correct/total}\")"],"metadata":{"id":"OOa6fy2oUue4"},"execution_count":null,"outputs":[]}]}